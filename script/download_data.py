#!/usr/bin/env python3
"""
Script de t√©l√©chargement des donn√©es depuis Kaggle
√Ä ex√©cuter depuis /home/site/wwwroot/app/ sur Azure
"""

import os
import sys
import logging
import time

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def check_kaggle_credentials():
    """V√©rifie que les credentials Kaggle sont configur√©s"""
    username = os.getenv("KAGGLE_USERNAME")
    key = os.getenv("KAGGLE_KEY")
    
    if not username or not key:
        logger.error("‚ùå Variables d'environnement Kaggle manquantes:")
        logger.error("   - KAGGLE_USERNAME")
        logger.error("   - KAGGLE_KEY")
        logger.error("üí° Configurez-les dans Azure App Service > Configuration > Application Settings")
        return False
    
    logger.info(f"‚úÖ Credentials Kaggle trouv√©s pour l'utilisateur: {username}")
    return True

def download_sentiment140_data():
    """
    T√©l√©charge le dataset Sentiment140 depuis Kaggle
    """
    # CORRECTION : Depuis /app/, le chemin est simplement "data"
    data_dir = "data"
    data_path = os.path.join(data_dir, "training.1600000.processed.noemoticon.csv")
    
    logger.info(f"üîç R√©pertoire de travail actuel : {os.getcwd()}")
    logger.info(f"üéØ Chemin cible des donn√©es : {data_path}")
    
    # V√©rifier si le fichier existe d√©j√†
    if os.path.exists(data_path):
        file_size = os.path.getsize(data_path) / (1024*1024)
        logger.info(f"‚úÖ Fichier d√©j√† pr√©sent : {data_path} ({file_size:.1f} MB)")
        return data_path
    
    # V√©rifier les credentials
    if not check_kaggle_credentials():
        raise Exception("Credentials Kaggle manquants")
    
    # Cr√©er le dossier de donn√©es s'il n'existe pas
    os.makedirs(data_dir, exist_ok=True)
    logger.info(f"üìÅ Dossier cr√©√©/v√©rifi√© : {data_dir}")
    
    try:
        # Initialiser l'API Kaggle
        logger.info("üîë Authentification Kaggle...")
        from kaggle.api.kaggle_api_extended import KaggleApi
        
        api = KaggleApi()
        api.authenticate()
        logger.info("‚úÖ Authentification Kaggle r√©ussie")
        
        # T√©l√©charger le dataset avec retry
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"üì• T√©l√©chargement du dataset Sentiment140 (tentative {attempt + 1}/{max_retries})...")
                
                api.dataset_download_file(
                    "kazanova/sentiment140",
                    file_name="training.1600000.processed.noemoticon.csv",
                    path=data_dir
                )
                
                if os.path.exists(data_path):
                    file_size = os.path.getsize(data_path) / (1024*1024)
                    logger.info(f"‚úÖ T√©l√©chargement termin√© : {data_path} ({file_size:.1f} MB)")
                    return data_path
                else:
                    raise Exception("Fichier non cr√©√© apr√®s t√©l√©chargement")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Tentative {attempt + 1} √©chou√©e : {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)  # Attendre 5s avant retry
                else:
                    raise
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du t√©l√©chargement : {e}")
        raise

def verify_data_integrity():
    """
    V√©rifie l'int√©grit√© des donn√©es t√©l√©charg√©es
    """
    data_path = os.path.join("data", "training.1600000.processed.noemoticon.csv")
    
    if not os.path.exists(data_path):
        logger.error(f"‚ùå Fichier non trouv√© : {data_path}")
        return False
    
    # V√©rifier la taille du fichier
    file_size = os.path.getsize(data_path)
    file_size_mb = file_size / (1024*1024)
    logger.info(f"üìä Taille du fichier : {file_size_mb:.1f} MB")
    
    # Le fichier devrait faire environ 238 MB
    if file_size_mb < 200:
        logger.warning(f"‚ö†Ô∏è  Fichier trop petit ({file_size_mb:.1f} MB), t√©l√©chargement possiblement incomplet")
        return False
    
    # V√©rification basique du contenu
    try:
        import pandas as pd
        col_names = ["target", "ids", "date", "flag", "user", "text"]
        df = pd.read_csv(data_path, encoding="latin-1", names=col_names, header=None, nrows=10)
        
        logger.info(f"‚úÖ Fichier lisible, {len(df)} lignes test√©es")
        logger.info(f"üìà Colonnes d√©tect√©es : {list(df.columns)}")
        
        # V√©rifier les valeurs target attendues
        unique_target


# #!/usr/bin/env python3
# """
# Script de t√©l√©chargement des donn√©es depuis Kaggle
# √Ä ex√©cuter AVANT de lancer l'API
# """

# import os
# import logging
# from kaggle.api.kaggle_api_extended import KaggleApi

# # Configuration du logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# def download_sentiment140_data():
#     """
#     T√©l√©charge le dataset Sentiment140 depuis Kaggle
#     """
#     # D√©finition du chemin vers le fichier CSV
#     data_dir = os.path.join("app", "data")
#     data_path = os.path.join(data_dir, "training.1600000.processed.noemoticon.csv")
    
#     # V√©rifier si le fichier existe d√©j√†
#     if os.path.exists(data_path):
#         logger.info(f"‚úÖ Fichier d√©j√† pr√©sent : {data_path}")
#         return data_path
    
#     # Cr√©er le dossier de donn√©es s'il n'existe pas
#     os.makedirs(data_dir, exist_ok=True)
#     logger.info(f"üìÅ Dossier cr√©√© : {data_dir}")
    
#     try:
#         # Initialiser l'API Kaggle
#         logger.info("üîë Authentification Kaggle...")
#         api = KaggleApi()
#         api.authenticate()
#         logger.info("‚úÖ Authentification Kaggle r√©ussie")
        
#         # T√©l√©charger le dataset
#         logger.info("üì• T√©l√©chargement du dataset Sentiment140...")
#         api.dataset_download_file(
#             "kazanova/sentiment140",
#             file_name="training.1600000.processed.noemoticon.csv",
#             path=data_dir
#         )
        
#         logger.info(f"‚úÖ T√©l√©chargement termin√© : {data_path}")
#         return data_path
        
#     except Exception as e:
#         logger.error(f"‚ùå Erreur lors du t√©l√©chargement : {e}")
#         raise

# def verify_data_integrity():
#     """
#     V√©rifie l'int√©grit√© des donn√©es t√©l√©charg√©es
#     """
#     data_path = os.path.join("app", "data", "training.1600000.processed.noemoticon.csv")
    
#     if not os.path.exists(data_path):
#         logger.error(f"‚ùå Fichier non trouv√© : {data_path}")
#         return False
    
#     # V√©rifier la taille du fichier
#     file_size = os.path.getsize(data_path)
#     logger.info(f"üìä Taille du fichier : {file_size / (1024*1024):.1f} MB")
    
#     # V√©rification basique du contenu
#     try:
#         import pandas as pd
#         col_names = ["target", "ids", "date", "flag", "user", "text"]
#         df = pd.read_csv(data_path, encoding="latin-1", names=col_names, header=None, nrows=5)
#         logger.info(f"‚úÖ Fichier lisible, colonnes : {list(df.columns)}")
#         logger.info(f"üìà Aper√ßu des premi√®res lignes :\n{df[['target', 'text']].head()}")
#         return True
#     except Exception as e:
#         logger.error(f"‚ùå Erreur lors de la v√©rification : {e}")
#         return False

# if __name__ == "__main__":
#     logger.info("üöÄ D√©marrage du script de t√©l√©chargement des donn√©es")
    
#     try:
#         # T√©l√©charger les donn√©es
#         data_path = download_sentiment140_data()
        
#         # V√©rifier l'int√©grit√©
#         if verify_data_integrity():
#             logger.info("üéâ Donn√©es pr√™tes pour l'API !")
#         else:
#             logger.error("‚ùå Probl√®me avec les donn√©es t√©l√©charg√©es")
            
#     except Exception as e:
#         logger.error(f"üí• √âchec du script : {e}")
#         exit(1)